{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roopasree-Pachipulusu/-Roopasree-Pachipulusu-.github.io/blob/main/Identification_of_cotton_species.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNveqeA1KXGy"
      },
      "source": [
        "# Step 1: Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTvDNSILZoN9",
        "outputId": "20f58343-cb3e-4ebd-e0ad-d2e103aa3621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 12396, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 12396 (delta 4), reused 8 (delta 1), pack-reused 12380\u001b[K\n",
            "Receiving objects: 100% (12396/12396), 12.17 MiB | 28.78 MiB/s, done.\n",
            "Resolving deltas: 100% (8547/8547), done.\n",
            "/content/yolov5\n",
            "\u001b[K     |████████████████████████████████| 596 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 178 kB 40.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 42.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 42.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 66.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[?25h  Building wheel for roboflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\n",
            "roboflow 0.2.7 requires Pillow==8.4.0, but you have pillow 7.1.2 which is incompatible.\u001b[0m\n",
            "Setup complete. Using torch 1.11.0+cu113 (CPU)\n"
          ]
        }
      ],
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP6USLgz2f0r"
      },
      "source": [
        "# Step 2: Assemble Our Dataset\n",
        "\n",
        "In order to train our custom model, we have assembeled our cotton image dataset of representative images with bounding box annotations around the objects that we want to detect. \n",
        "In this part, we convert annotations into the format expected by YOLO v5.YOLO v5 expects annotations for each image in form of a .txt file where each line of the text file describes a bounding box.\n",
        "We have used Roboflow to Annotate and Assemble the dataset \n",
        "\n",
        "\n",
        "\n",
        "# Annotate\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1-0JJY8bbFO6FQoX-a2x4PWFCSAazu_Tp)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnpRZefUxK3u"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R6GEGjdxVMM"
      },
      "source": [
        "# Step 3: Assemble Our Dataset\n",
        "\n",
        "# Pre-processing and Data Agumentation \n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1mNcCPNIupuiJxo7qLUaemmCALZ8lw-9d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPqmJwPI0vF7"
      },
      "source": [
        "# Step 4: Assemble Our Dataset\n",
        "#Partitioning the data set\n",
        "Next we partition the dataset into train, validation, and test sets containing 80%, 10%, and 10% of the data, respectively. \n",
        "\n",
        "\n",
        "* train - It is the set of data that is used to train and make the model learn the hidden features/patterns in the data.\n",
        "\n",
        "* validation -The validation set is a set of data, separate from the training set, that is used to validate our model performance during training.\n",
        "This validation process gives information that helps us tune the model’s hyperparameters and configurations accordingly. It is like a critic telling us whether the training is moving in the right direction or not.\n",
        "The model is trained on the training set, and, simultaneously, the model evaluation is performed on the validation set after every epoch.\n",
        "The main idea of splitting the dataset into a validation set is to prevent our model from overfitting i.e., the model becomes really good at classifying the samples in the training set but cannot generalize and make accurate classifications on the data it has not seen before. \n",
        "\n",
        "* test - The test set is a separate set of data used to test the model after completing the training.\n",
        "![picture](https://drive.google.com/uc?id=1GTe-GAXIptegmUMUyz3bR3ajaEeEWOeh)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2wGvjd4Z_92",
        "outputId": "bd99c78f-cb11-42e7-f173-3183ae78539c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=ultralytics\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upRjzvd2j2Nz",
        "outputId": "648d2403-3a7c-405d-d6e2-d996999354e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jjT5uIHo6l5"
      },
      "outputs": [],
      "source": [
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/drive/MyDrive/datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "FwJcaoPGF4VI",
        "outputId": "11a9e00a-389f-484b-b7e8-d8ac40c8e5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.7/dist-packages (0.2.7)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.3.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.20.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2021.5.30)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.64.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.9.1)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: opencv-python-headless==4.1.2.30 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.1.2.30)\n",
            "Collecting Pillow==8.4.0\n",
            "  Using cached Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Requirement already satisfied: urllib3==1.26.6 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.26.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.28.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.0.12)\n",
            "Installing collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-8.4.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in /content/drive/MyDrive/datasets/bt-cotton-5 to yolov5pytorch: 100% [40995801 / 40995801] bytes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Dataset Version Zip to /content/drive/MyDrive/datasets/bt-cotton-5 in yolov5pytorch:: 100%|██████████| 4258/4258 [15:21<00:00,  4.62it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"ZYZwQ6u1cXtNANHiAdAS\")\n",
        "project = rf.workspace(\"cotton-nqp2x\").project(\"bt-cotton\")\n",
        "dataset = project.version(5).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7yAi9hd-T4B"
      },
      "source": [
        "# Step 5: Train Our Custom YOLOv5 model\n",
        "\n",
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
        "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
        "- **cache:** cache images for faster training\n",
        "- **hyp**  YAML file that describes hyperparameter choices. For examples of how to define hyperparameters, see data/hyp.scratch.yaml. If unspecified, the file data/hyp.scratch.yaml is used\n",
        "-**Data Config File** \n",
        "Details for the dataset we want to train your model on are defined by the data config YAML file. The following parameters have to be defined in a data config file:\n",
        "  1.   train, test, and val: Locations of train, test, and validation images.\n",
        "  2. nc: Number of classes in the dataset. \n",
        "  3. names: Names of the classes in the dataset. The index of the classes in this list would be used as an identifier for the class names in the code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJR28l1C34kv"
      },
      "outputs": [],
      "source": [
        "# !python train.py --img 416 --batch 16 --epochs 100 --data '../data.yaml' --cfg ./models/custom_yolov5s.yaml --weights '' --name yolov5s_results --cache --project '../gdrive/MyDrive/' --name 'backup'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaFNnxLJbq4J",
        "outputId": "90697d7f-8614-4a06-f257-c5bed823e4d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data={dataset.location}/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=500, batch_size=20, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=/content/drive/MyDrive/, name=backup_weights, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 666, in <module>\n",
            "    main(opt)\n",
            "  File \"train.py\", line 536, in main\n",
            "    check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(opt.project)  # checks\n",
            "  File \"/content/yolov5/yolov5/utils/general.py\", line 435, in check_file\n",
            "    assert len(files), f'File not found: {file}'  # assert file was found\n",
            "AssertionError: File not found: {dataset.location}/data.yaml\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 416 --batch 20 --epochs 50 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIRLQOlA14A"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance\n",
        "Training losses and performance metrics are saved to Tensorboard and also to a logfile.\n",
        "\n",
        "If you are new to these metrics, the one you want to focus on is `mAP_0.5` - learn more about mean average precision [here](https://blog.roboflow.com/mean-average-precision/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmS7_TXFsT3"
      },
      "source": [
        "#Run Inference  With Trained Weights\n",
        "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWjjiBcic3Vz",
        "outputId": "e3cf56e1-ecb7-4ec9-da88-b5341696f087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/best.pt'], source=/content/drive/MyDrive/datasets/bt-cotton-5/test/images, data=data/coco128.yaml, imgsz=[416, 416], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v6.1-253-g75bbaa8 Python-3.7.13 torch-1.11.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7020913 parameters, 0 gradients\n",
            "image 1/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a-115_jpeg.rf.46e69dc3ffc91d96a9c54926095fd10a.jpg: 416x416 1 G.herbaceum, Done. (0.011s)\n",
            "image 2/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a-115_jpeg.rf.ae24b4f82f5873356871cf2546450b33.jpg: 416x416 Done. (0.011s)\n",
            "image 3/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a-115_jpeg.rf.b2389aab139bbbf5ee9dc6ead8281c8d.jpg: 416x416 Done. (0.010s)\n",
            "image 4/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a-115_jpeg.rf.c4c2af93f288a3a192a3ceb64e044fd8.jpg: 416x416 Done. (0.011s)\n",
            "image 5/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a-115_jpeg.rf.d579e72c1a4cdfae071c5b6bb97b04eb.jpg: 416x416 Done. (0.011s)\n",
            "image 6/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a-123_jpeg.rf.dc67d8e8f5dc84bdc630d4799fc707f8.jpg: 416x416 Done. (0.010s)\n",
            "image 7/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a18_jpg.rf.f82a8934b0fb1d81acd50d4a85dbfbfa.jpg: 416x416 Done. (0.010s)\n",
            "image 8/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a19_jpg.rf.2757c78a989a1c68995691ca59a6ed4b.jpg: 416x416 Done. (0.010s)\n",
            "image 9/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a40_jpg.rf.a11bc1ea8148acccf99802082a1f877b.jpg: 416x416 Done. (0.011s)\n",
            "image 10/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a42_jpg.rf.08794a69faa2011a035415af27338e98.jpg: 416x416 Done. (0.010s)\n",
            "image 11/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a42_jpg.rf.74612cd57e69a457bab848e47a382dbc.jpg: 416x416 Done. (0.010s)\n",
            "image 12/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a42_jpg.rf.eeb7c0ab27b76a01e89a6ef08d5ed65e.jpg: 416x416 Done. (0.019s)\n",
            "image 13/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a5_jpg.rf.3e87a2e1f0d159b652b01d9aea04f4a1.jpg: 416x416 Done. (0.011s)\n",
            "image 14/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a5_jpg.rf.943b6ce49513ea1fdf8732ff786c7cc0.jpg: 416x416 Done. (0.011s)\n",
            "image 15/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a63_jpg.rf.c2d5e3b472169a4e2eb570ec4356cede.jpg: 416x416 Done. (0.010s)\n",
            "image 16/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a69_jpg.rf.2acf06d189d9f6724a58d802686a4c60.jpg: 416x416 Done. (0.010s)\n",
            "image 17/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a69_jpg.rf.44d540449f5045c48508189010da5066.jpg: 416x416 Done. (0.012s)\n",
            "image 18/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a69_jpg.rf.62d702491be92e8ae5a3c45e1e33d2a7.jpg: 416x416 Done. (0.011s)\n",
            "image 19/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a69_jpg.rf.8e75dffa27379c8593a8d35560653129.jpg: 416x416 Done. (0.011s)\n",
            "image 20/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a69_jpg.rf.96b36909aab2408c5b08d7ff107e9aee.jpg: 416x416 Done. (0.012s)\n",
            "image 21/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a69_jpg.rf.9ce8f1a7b56192fdf231923949ba7f4a.jpg: 416x416 Done. (0.010s)\n",
            "image 22/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a69_jpg.rf.c4951196c592f892ee4fbfe2cc49e14c.jpg: 416x416 1 G.arboreum, Done. (0.018s)\n",
            "image 23/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a69_jpg.rf.d88f3b3f27ad287cb4e3d09dd2547e52.jpg: 416x416 Done. (0.012s)\n",
            "image 24/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a69_jpg.rf.e3d311c290397bac8e03203269c061bf.jpg: 416x416 1 G.barbadense, Done. (0.011s)\n",
            "image 25/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a70_jpg.rf.5b71b4339d490bc25b4f9538309677dd.jpg: 416x416 1 G.arboreum, 1 G.hirsitum, Done. (0.011s)\n",
            "image 26/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a86_jpg.rf.47e64af740dc6aad26ac7c05f8cf9072.jpg: 416x416 Done. (0.012s)\n",
            "image 27/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/a86_jpg.rf.680f4e883df56c381058956ea6e71593.jpg: 416x416 Done. (0.012s)\n",
            "image 28/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/b-72_jpeg.rf.551663d6bc87796fe6425f1ca17eccec.jpg: 416x416 1 G.arboreum, Done. (0.011s)\n",
            "image 29/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h17_jpg.rf.097b6266b345427609f825a74437714c.jpg: 416x416 1 G.hirsitum, Done. (0.011s)\n",
            "image 30/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h17_jpg.rf.292a4273e57cf4f6ebfd2d057676137a.jpg: 416x416 1 G.hirsitum, Done. (0.012s)\n",
            "image 31/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h17_jpg.rf.6554eda9e438379433a55fc0e554e7fb.jpg: 416x416 1 G.hirsitum, Done. (0.011s)\n",
            "image 32/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h17_jpg.rf.883d5347943c9ca6816ddd50b1d02fb0.jpg: 416x416 1 G.hirsitum, Done. (0.015s)\n",
            "image 33/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h17_jpg.rf.e39191272eb16f8f1584b273491255f6.jpg: 416x416 1 G.hirsitum, Done. (0.011s)\n",
            "image 34/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h20_jpg.rf.bbb72a9b0629031a0c30aa19d378b6d2.jpg: 416x416 1 G.arboreum, 1 G.hirsitum, Done. (0.011s)\n",
            "image 35/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h26_jpg.rf.3b719921f358cad4fc801b67883e82eb.jpg: 416x416 1 G.hirsitum, Done. (0.012s)\n",
            "image 36/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h32_jpg.rf.9c7a195d6fb2e893a16328e888283e72.jpg: 416x416 1 G.hirsitum, Done. (0.013s)\n",
            "image 37/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h34_jpg.rf.64a6d268a4d6f313eb818f5b5adbe0dc.jpg: 416x416 1 G.herbaceum, Done. (0.012s)\n",
            "image 38/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h37_jpg.rf.8534e59e1b43cd24494144988657bdf7.jpg: 416x416 1 G.arboreum, 1 G.hirsitum, Done. (0.011s)\n",
            "image 39/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h40_jpg.rf.d593f72bbd84c9b7bc18ef07f3b598cd.jpg: 416x416 1 G.hirsitum, Done. (0.014s)\n",
            "image 40/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h5_jpg.rf.1ccce456ecc3ffa64a8fff57a1752dcd.jpg: 416x416 Done. (0.011s)\n",
            "image 41/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/h5_jpg.rf.ad44b69ff551e269d001a33cc983371c.jpg: 416x416 Done. (0.012s)\n",
            "image 42/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr-104_jpg.rf.48d9e1970da26754e2b30a58a97bdb37.jpg: 416x416 1 G.arboreum, Done. (0.012s)\n",
            "image 43/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr-110_jpg.rf.82c69bb9a6c06874a8c2b99e8a03f474.jpg: 416x416 1 G.herbaceum, Done. (0.012s)\n",
            "image 44/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr-113_jpg.rf.e955c4fd41d1e51c7bb6b6c61c292ca0.jpg: 416x416 1 G.herbaceum, Done. (0.011s)\n",
            "image 45/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr-118_jpg.rf.ebd5789b4fde37fa122095aa05986b9a.jpg: 416x416 1 G.arboreum, Done. (0.011s)\n",
            "image 46/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr11_jpg.rf.1ebadbaee5e98ae51a66423d994dc026.jpg: 416x416 1 G.arboreum, 1 G.herbaceum, Done. (0.011s)\n",
            "image 47/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr11_jpg.rf.73430796b662d81a3a8e025fdb4eb32d.jpg: 416x416 1 G.arboreum, Done. (0.011s)\n",
            "image 48/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr13_jpg.rf.a62921e3baf074edfb0577c93a543468.jpg: 416x416 1 G.herbaceum, Done. (0.014s)\n",
            "image 49/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr18_jpg.rf.0e0ed84bb9a6b1bac9f913853ff1d239.jpg: 416x416 1 G.herbaceum, Done. (0.011s)\n",
            "image 50/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr18_jpg.rf.10239bc09289da5e79add7ef1d9b0594.jpg: 416x416 1 G.barbadense, 1 G.herbaceum, Done. (0.012s)\n",
            "image 51/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr18_jpg.rf.19d2a14f7716490afa5853eea533bec4.jpg: 416x416 1 G.herbaceum, Done. (0.013s)\n",
            "image 52/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr18_jpg.rf.29e6523ed88f8545596eb71da9c45ee9.jpg: 416x416 1 G.herbaceum, Done. (0.013s)\n",
            "image 53/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr18_jpg.rf.3b05137ae6efa152633a5775d429fcc8.jpg: 416x416 1 G.herbaceum, Done. (0.012s)\n",
            "image 54/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr18_jpg.rf.3b4ded9309a9cbd8db0d255a499c45a9.jpg: 416x416 1 G.barbadense, Done. (0.012s)\n",
            "image 55/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr18_jpg.rf.3e393bcefebade612ce96c5b67dd5914.jpg: 416x416 1 G.arboreum, 1 G.herbaceum, Done. (0.012s)\n",
            "image 56/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr18_jpg.rf.8062147dbce642822646aaddf879056a.jpg: 416x416 1 G.herbaceum, Done. (0.016s)\n",
            "image 57/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr20_jpg.rf.189d74bb92e8e532134a9fbbdd450508.jpg: 416x416 1 G.herbaceum, Done. (0.014s)\n",
            "image 58/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr20_jpg.rf.72752b262a9a8e414957aa29e4e07a5d.jpg: 416x416 1 G.herbaceum, Done. (0.012s)\n",
            "image 59/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr20_jpg.rf.76d3965648a1bae5ec34ba00562533a1.jpg: 416x416 1 G.herbaceum, Done. (0.011s)\n",
            "image 60/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr20_jpg.rf.d479cfd32fda4a02d6e515676cb3119b.jpg: 416x416 1 G.herbaceum, Done. (0.017s)\n",
            "image 61/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr22_jpg.rf.743954293d2a65ee14a00ab0d4167cb2.jpg: 416x416 1 G.barbadense, Done. (0.012s)\n",
            "image 62/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr27_jpg.rf.44d7991b16ccdbf5f4d662f1d529fd28.jpg: 416x416 1 G.herbaceum, Done. (0.012s)\n",
            "image 63/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr46_jpg.rf.050844bd1a0cde5e65db2145bd9ac769.jpg: 416x416 Done. (0.012s)\n",
            "image 64/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr46_jpg.rf.0f112311c0122e7e6bebbabd4bf9e5d0.jpg: 416x416 Done. (0.012s)\n",
            "image 65/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr46_jpg.rf.6a95cfba0e8cff704802285a243eeb53.jpg: 416x416 Done. (0.012s)\n",
            "image 66/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr46_jpg.rf.812e333107f263996388e2bc3c11a999.jpg: 416x416 Done. (0.012s)\n",
            "image 67/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr46_jpg.rf.a922712658e34e2f487633851c58e445.jpg: 416x416 Done. (0.012s)\n",
            "image 68/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr46_jpg.rf.b88ed3a51e485e2f33f8db3a9a4ec456.jpg: 416x416 Done. (0.012s)\n",
            "image 69/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr46_jpg.rf.c96f98a5860c72d6b9e3785ba4973698.jpg: 416x416 Done. (0.015s)\n",
            "image 70/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr46_jpg.rf.cc5e8544031741929fd7af34d04dffa5.jpg: 416x416 Done. (0.012s)\n",
            "image 71/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr46_jpg.rf.e6e6f3009ab1dda92c81b7ee7866cbb7.jpg: 416x416 Done. (0.012s)\n",
            "image 72/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr76_jpg.rf.b5a8fb671ca026c68a9ffa3de2050fb0.jpg: 416x416 1 G.herbaceum, Done. (0.011s)\n",
            "image 73/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr79_jpg.rf.c6b3d58a1e8db4c173d1e43a16498e5d.jpg: 416x416 1 G.herbaceum, Done. (0.011s)\n",
            "image 74/74 /content/drive/MyDrive/datasets/bt-cotton-5/test/images/hr9_jpg.rf.79714938286cd622416c582fa625f708.jpg: 416x416 1 G.arboreum, Done. (0.011s)\n",
            "Speed: 0.4ms pre-process, 11.9ms inference, 0.7ms NMS per image at shape (1, 3, 416, 416)\n",
            "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 416 --conf 0.1 --source /content/drive/MyDrive/datasets/bt-cotton-5/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbUn4_b9GCKO"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob(''): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dHcni6CJYt"
      },
      "source": [
        "# Conclusion and Further enhancement\n",
        "\n",
        "\n",
        "\n",
        "To improve you model's performance, we recommend first interating on your datasets coverage and quality. See this guide for [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n",
        "\n",
        "To deploy your model to an application, see this guide on [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n",
        "\n",
        "Once your model is in production, you will want to continually iterate and improve on your dataset and model via [active learning](https://blog.roboflow.com/what-is-active-learning/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "7iiObB2WCMh6",
        "outputId": "4721539e-b11c-4cbe-d092-9be4a7e5a5d7"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_c6ebade4-43a9-4b5e-8527-ae2fd739d5f2\", \"best.pt\", 14347189)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#export your model's weights for future use\n",
        "from google.colab import files\n",
        "files.download('./runs/train/exp/weights/best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNn-obvOGITm"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Identification of cotton species.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}